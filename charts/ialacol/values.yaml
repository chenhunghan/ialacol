replicas: 1

deployment:
  image: quay.io/chenhunghan/ialacol:latest
  env:
    # 
    # the model repo to download from, runtime defaults to `rustformers/pythia-ggml`
    # https://huggingface.co/rustformers/mpt-7b-ggml
    DEFAULT_MODEL_HG_REPO_ID: rustformers/mpt-7b-ggml
    # the file name of the model binary in the repo, runtime defaults to `pythia-70m-q4_0.bin`
    DEFAULT_MODEL_FILE: mpt-7b-q4_0.bin
    # the metadata file name of the model in the repo, runtime defaults to `pythia-70m-q4_0.meta`
    DEFAULT_MODEL_META: mpt-7b-q4_0.meta
    # start of optional env vars
    # 
    # log level, runtime defaults to `INFO`
    LOGGING_LEVEL: DEBUG
    # the model cache directory, defaults to `models`, i.e., `/app/cache` in the container
    MODELS_FOLDER: models
    # the cache directory (for hf artifacts), defaults to `models`, i.e., `/app/cache` in the container
    CACHE_FOLDER: cache
resources:
  {}
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

cache:
  persistence:
    size: 10Gi
    accessModes:
      - ReadWriteOnce
    # Optional. e.g. "gp2-unencrypted"
    storageClass: ~
# the path to mount the cache volume on the container
cacheMountPath: /app/cache

model:
  persistence:
    size: 24Gi
    accessModes:
      - ReadWriteOnce
    # Optional. e.g. "gp2-unencrypted"
    storageClass: ~
# the path to mount the model volume on the container
modelMountPath: /app/models

service:
  type: ClusterIP
  port: 80
  annotations: {}
  # If using an AWS load balancer, you'll need to override the default 60s load balancer idle timeout
  # service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "1200"
nodeSelector: {}
tolerations: []
  # e.g.
  # - key: "ai"
  #   operator: "Exists"
  #   effect: "NoSchedule"
affinity: {}
  # e.g.
  # nodeAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #     - matchExpressions:
  #       - key: nodegroup-ai
  #         operator: In
  #         values:
  #         - "true"
